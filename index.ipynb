{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Max said he would grade this\n",
    "# Optimal grokking: Exploring TrueGrad Adam implementations and normalized singular values\n",
    "\n",
    "## Introduction\n",
    "What is grokking? [Grokking](https://arxiv.org/pdf/2201.02177.pdf) is a curious phenomenon in neural networks. It in some sense spits in the face of classic understanding of training models. Rather than describe it, as they say, a picture is worth 1,000 words.\n",
    "\n",
    "</br>\n",
    "\n",
    "![](./grokking.png)\n",
    "\n",
    "Taken from the research paper linked earlier, grokking is the behavior observed on some simple problems that long past overfitting, models almost suddenly generalize. And by long past overfitting, in this example we mean *long* past, nearly 4 orders of magnitude!\n",
    "\n",
    "## Its not actually that bad!\n",
    "Okay, so 4 orders of magnitude seem completely untenable, but this was the worst case described in the paper. Lets talk about the problem discussed in the paper a bit.\n",
    "</br>\n",
    "\n",
    "### So what is the problem described in the paper? What does the paper do?\n",
    "### The problem (simplified)\n",
    "Modular division </br>\n",
    "* Choose some prime $P$, the paper chose $P=97$\n",
    "* Generate all equations of the form $a+b \\equiv c$ (mod $P)$\n",
    "    * $a,b,c \\in \\mathbb{Z}^{0\\leq P}$\n",
    "* This will generate $N_{data} = P*P$ equations\n",
    "    * Split the data into training and validating\n",
    "* Train a standard TransformerDecoder of the following structure\n",
    "    * Embedding layer with positional encoding (functionally an encoder)\n",
    "    * 2 Layers\n",
    "        * width 128\n",
    "        * 4 attention heads\n",
    "* With an Adam optimizer having parameters\n",
    "    * learning rate $10^{-3}$\n",
    "    * weight decay $10^{-2}$ - they said 1, but pretty sure they meant $1e-2$... 1 is insane\n",
    "    * $B_1 = 0.9$, $B_2=0.98$\n",
    "\n",
    "## The problem actually described in the paper\n",
    "The problem described above is a lemma of what the paper actually does. This section is not required to understand anything we did, but we would be remiss if we did not talk about the finer details of the paper. </br>\n",
    "* Not just modular division\n",
    "    * $a \\circ b \\equiv c$(mod $P$) \n",
    "    * For the following ops\n",
    "        *  $a \\circ b$ $ = a + b$ (mod $P$) for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = a- b$ (mod $P$) for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = [a/b$ (mod $P$) for $0 \\leq a \\le P$, $0 \\le b \\le P$\n",
    "        *  $a \\circ b$ $ = [a/b$ (mod $P$) if $b$ is odd, otherwise $a-b$ (mod $P$)] for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = a^2 + b^2$ (mod $P$) for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = a^2 + ab + b^2$ (mod $P$) for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = a^2 + ab + b^2 + a$ (mod $P$) for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = a^3 + ab$ (mod $P$) for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = a^3 + ab^2 + b$ (mod $P$) for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = a * b$ for $a,b \\in \\mathbb{S}^5$\n",
    "        *  $a \\circ b$ $ = a * b * a^{-1}$ for $a,b \\in \\mathbb{S}^5$\n",
    "        *  $a \\circ b$ $ = a * b * a$ for $a,b \\in \\mathbb{S}^5$\n",
    "* This means every input was 5 tokens\n",
    "    * \"a\", \"op\", \"b\", \"=\", \"c\"\n",
    "* Generate all equations of this form for some specific op\n",
    "* Convert the 5 char arrays to 5 int arrays\n",
    "* Split the data as before\n",
    "* The transformer structure is unchanged\n",
    "* They used a variety of optimization techniques and hyperparameter tuning\n",
    "    * Minibatching and full batching\n",
    "    * 10 warm up updates of mini batch size $[512, \\frac{N_{trainingData}}{2}]$\n",
    "    * optimization budget of $10^5$ gradient updates - I read this as steps.\n",
    "    * learning rate $3e-4$, $3e-3$\n",
    "    * weight decay same as before\n",
    "    * Gaussian noise on weights\n",
    "    * residual dropout 0.1\n",
    "* They also put outliers in the dataset to see how this effected grokking\n",
    "\n",
    "Importantly this reperesents a more full scope of what the study did. </br>\n",
    "For our purposes we will just take some of their better presets as a base model.\n",
    "\n",
    "## Results figure\n",
    "![](./grokkingResults.png)\n",
    "* Yes this is the worst graph in terms of labeling ever\n",
    "* No the figure explanation does not help\n",
    "* Below we talk about the key findings of the paper mostly ignoring this graph\n",
    "\n",
    "## Key Results\n",
    "* Adam is seemingly very important to grokking, at least momentum optimizers\n",
    "* Grokking didn't happen until the singular values of weights became small\n",
    "* Minibatching is superior\n",
    "* Weight decay is *extremely* *extremely* important\n",
    "* They didn't give precise results as far as We could tell\n",
    "    * No statement that is this problem, this training data split, N optimization steps to train, M optimizations steps to grok\n",
    "    * There exists some parameters such that grokking happens within an order of magnitude\n",
    "\n",
    "\n",
    "\n",
    "## What we took from this as our base model\n",
    "* The transformer model with dropout\n",
    "* Adam optimizer \n",
    "    * $lr = 3e-4$\n",
    "    * $weightDecay = 1e-2$\n",
    "    * $B_1 = 0.9$, $B_2=0.98$\n",
    "* Full batch training\n",
    "\n",
    "Essentially this represented their best presets minus minibatching which preformed significantly better in their training. \n",
    "\n",
    "\n",
    "\n",
    "### Why didn't we do minibatching?\n",
    "* We believe on intuition that TrueGrad Adam should be much more stable\n",
    "* The best way to show this is to use the \"worst\" case for the optimizer\n",
    "* Still good presets for Adam, but a hard situation\n",
    "\n",
    "## Paper Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used libraries for complete implementation\n",
    "* [pytorch](https://github.com/pytorch/pytorch)\n",
    "* [Tensorflow](https://github.com/tensorflow/tensorflow) (Just for mnist dataset)\n",
    "* [numpy](https://github.com/numpy/numpy)\n",
    "* [TrueGrad](https://github.com/ClashLuke/TrueGrad)\n",
    "* [fasth](https://github.com/AlexanderMath/fasth)\n",
    "* [pandas](https://github.com/pandas-dev/pandas)\n",
    "* [matplotlib](https://matplotlib.org/)\n",
    "#### All neccessary imports for paper re-implementation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from typing import Optional, List, Tuple\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preset parameters from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 97\n",
    "DEVICE = \"cuda\"\n",
    "D_MODEL = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "\n",
    "Unlike the paper we generate the smaller verison so. So our data looks as follows\n",
    "* $x \\in x_{train}$ x = \\[10, 17, 0\\]\n",
    "* corresponding y = \\[10, 17, (10 + 17) % p \\]\n",
    "* Essentially x is just y without the answer\n",
    "We split the data into training and validation to the specified amount of training data.\n",
    "* $x_{train}, y_{train}$ are shape (n_training_data, 3)\n",
    "* $x_{test}, y_{test}$ are shape ($P*P$ - n_training_data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(n_training_data: int, p: int, device: str = \"cuda\"):\n",
    "    # generate all possible equations for mod p\n",
    "    all_data = []\n",
    "    for i in range(p):\n",
    "        for j in range(p):\n",
    "           all_data.append([i,j,(i+j)%p])\n",
    "    \n",
    "    all_data = np.array(all_data)\n",
    "    indices = np.random.permutation(all_data.shape[0])\n",
    "    train_indices = indices[:n_training_data]\n",
    "    valid_indices = indices[n_training_data:]\n",
    "    \n",
    "    input_seq: np.ndarray = all_data.copy()\n",
    "    output_seq = input_seq.copy()\n",
    "    input_seq[:, -1] = 0 # don't include answers\n",
    "\n",
    "    train_x = torch.tensor(input_seq[train_indices]).long().to(device)\n",
    "    train_y = torch.tensor(output_seq[train_indices]).long().to(device)\n",
    "    valid_x = torch.tensor(input_seq[valid_indices]).long().to(device)\n",
    "    valid_y = torch.tensor(output_seq[valid_indices]).long().to(device)\n",
    "\n",
    "    return (train_x, train_y), (valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The embedding layer\n",
    "Pytorch doesn't seem to have an embedding with position encoding built in yet. It might seem overkill to implement but transformers are permutation invariant which on a symmetric problem might give us an unfair advantage.\n",
    "\n",
    "Our implementation is standard or positional encoding - we used the [following](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/) as reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithPE(torch.nn.Embedding):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, seq_len: int, N: int= 10_000,\n",
    "            padding_idx: Optional[int] = None, max_norm: Optional[float] = None,\n",
    "            norm_type: float = 2.0, scale_grad_by_freq: bool = False, sparse: bool=False,\n",
    "            device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None\n",
    "        ):\n",
    "        super().__init__(\n",
    "            num_embeddings, embedding_dim, padding_idx, max_norm, norm_type,\n",
    "            scale_grad_by_freq, sparse, device, dtype\n",
    "        )\n",
    "       \n",
    "        pe = torch.zeros((seq_len, embedding_dim))\n",
    "        buf = torch.arange(embedding_dim//2)\n",
    "        denom = torch.pow(N, (2*buf)/embedding_dim)\n",
    "        k = torch.arange(seq_len)\n",
    "        pe[k, ::2] = torch.sin(k.unsqueeze(1)/denom)\n",
    "        pe[k, 1::2] = torch.cos(k.unsqueeze(1)/denom)\n",
    "        # pe should not be a learnable parameter, a buffer not a parameter\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "             \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return F.embedding(\n",
    "            x, self.weight, self.padding_idx, self.max_norm,\n",
    "            self.norm_type, self.scale_grad_by_freq, self.sparse) + self.pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer Model\n",
    "Won't rehash the copying of the paper but will mention a few important things.\n",
    "The paper stipulated a standard decoder so this is what we built, this includes masking and layernorms.\n",
    "\n",
    "Also important is a decoder of width N will have an output layer with N neurons. We are classifying a number that is at most P which is num_embeddings. So we add a layer such that the final layer has num_embedding (P) outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, seq_length: int, num_embeddings: int, d_model: int, dim_feedforward: int = 2048):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embed = EmbeddingWithPE(\n",
    "            self.num_embeddings, self.d_model, seq_length)\n",
    "\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(self.d_model, nhead=4, dim_feedforward=dim_feedforward,\n",
    "                                        dropout=0.1, batch_first=True, norm_first=True)\n",
    "        decoder_norm = torch.nn.LayerNorm(self.d_model)\n",
    "\n",
    "        self.decoder = torch.nn.TransformerDecoder(\n",
    "            decoder_layer, num_layers=2, norm=decoder_norm)\n",
    "        self.linear = torch.nn.Linear(\n",
    "            self.d_model, self.num_embeddings, bias=False)\n",
    "        self.mask = Parameter(\n",
    "            torch.ones([seq_length, seq_length]).tril())\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.embed(x)\n",
    "        x = self.decoder.forward(x, torch.zeros_like(x), self.mask)\n",
    "        return self.linear(x)\n",
    "\n",
    "    def acc(self, prediction: Tensor, labels: Tensor):\n",
    "        return (torch.argmax(prediction, dim=-2) == labels).float().mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "Here we write a simple training loop to do full batch training of the model. We record the training losses and accuracie as well as the validation losses and accuracies. \n",
    "\n",
    "We use cross entropy because always use cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_train(\n",
    "    model: Transformer,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_x: Tensor,\n",
    "    train_y: Tensor, \n",
    "    valid_x: Tensor, \n",
    "    valid_y: Tensor,\n",
    "    epochs: int = 10_000,\n",
    "    quiet: bool = False\n",
    ") -> Tuple[ Tuple[List[Tensor], List[Tensor]],  Tuple[List[Tensor], List[Tensor]]  ]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Simple training method, does full batch training on transformer model\n",
    "    returns (train_accs, train_losses), (valid_accs, valid_losses) \n",
    "    where each list is length epochs\n",
    "    \"\"\"\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_losses: List[Tensor] = []\n",
    "    train_accs: List[Tensor] = []\n",
    "    validation_losses: List[Tensor] = []\n",
    "    validation_accs: List[Tensor] = []\n",
    "    for i in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(train_x)\n",
    "        print(output.shape)\n",
    "        # model.forward shape is (n_inputs, 3, 97)\n",
    "        # transpose to (n_inputs, 97, 3)\n",
    "        pred = output.transpose(-2, -1)\n",
    "        # -1: to not drop the dim\n",
    "        pred = pred[..., -1:] \n",
    "        label = train_y[:, -1:]\n",
    "\n",
    "        loss = F.cross_entropy(pred, label)\n",
    "        acc = model.acc(pred, label)\n",
    "        \n",
    "        # keep track of losses\n",
    "        train_accs.append(acc.item())\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_valid = model.forward(valid_x).transpose(-2, -1)[..., -1:]\n",
    "            label_valid = valid_y[:, -1:]\n",
    "            valid_loss = F.cross_entropy(pred_valid, label_valid)\n",
    "            valid_acc = model.acc(pred_valid, label_valid)\n",
    "            validation_accs.append(valid_acc.item())\n",
    "            validation_losses.append(valid_loss.item())\n",
    "        if i % 100 == 0 and not quiet:\n",
    "            print(f\"Epoch {i}: loss {loss.item():e}, training_accuracy {acc}, valid_acc {valid_acc:4f}, valid_loss: {valid_loss:4f}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return (train_accs, train_losses), (validation_accs, validation_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on a variety of parameters\n",
    "\n",
    "We want to generate an amount of results to compare so we do grid search on the following parameters.\n",
    "* N_epochs = 15,000\n",
    "* n_training_data = 800 + 100k $k \\in \\mathbb{Z}^{0 \\le 12}$\n",
    "* d_model = \\[64, 128, 256\\]\n",
    "* dim_feedforward = \\[256, 512, 1024, 2048\\]\n",
    "</br>\n",
    "\n",
    "This took 1.5 *days* of compute on an rtx 2060 12GB.\n",
    "We save all fully trained models to do later analysis on their singular values as well the train and validation losses and accuracies.\n",
    "\n",
    "The naming scheme for the saved files is\n",
    "* .csv or .model depending on if its the model state dict or the training accuracies losses etcetera\n",
    "* \\<n_training_data\\>_\\<d_model\\>_\\<dim_feedforward\\>\n",
    "* To not confuse anyone from earlier d_model is the same as width from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of not running this biaccident we put this behind a function\n",
    "def DO_NOT_RUN():\n",
    "    for n_training_data in range(800, 2001, 100):\n",
    "        # For consistency training data must be the same for the models of different params\n",
    "        (train_x, train_y), (valid_x, valid_y) = gen_data(n_training_data, P, device=DEVICE)\n",
    "\n",
    "        for d_model in [64, 128, 256]:\n",
    "            for dim_feedforward in [256, 512, 1024, 2048]:\n",
    "                model = Transformer(train_x.size(-1), P, d_model, dim_feedforward=dim_feedforward).to(DEVICE)\n",
    "\n",
    "                optimizer = torch.optim.Adam(\n",
    "                    model.parameters(),\n",
    "                    lr=3e-4,\n",
    "                    betas=[.9, .98],\n",
    "                    weight_decay=1e-2,\n",
    "                    eps=1e-8,\n",
    "                    amsgrad=False,\n",
    "                )\n",
    "                print(f\"Training model: n_train_data: {n_training_data}, d_model: {d_model} dim_feedforward: {dim_feedforward} \")\n",
    "                (acc, loss), (valid_acc, valid_loss) = simple_train(model, optimizer, train_x, train_y, valid_x, valid_y, quiet=False,epochs=15_000)\n",
    "                df_cols = [\"Training Accuracy\", \"Training Loss\", \"Validation Accuracy\", \"Validation Loss\"]\n",
    "                df = pd.DataFrame(list(zip(acc, loss, valid_acc, valid_loss)), columns=df_cols)\n",
    "                df.to_csv(f\"output/{n_training_data}_{d_model}_{dim_feedforward}.csv\")\n",
    "                torch.save(model.state_dict(), f\"output/{n_training_data}_{d_model}_{dim_feedforward}.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of accuracies\n",
    "\n",
    "First lets make a series plots as follows where <> denotes an immutable parameter\n",
    "* (training_data, \\<d_model=64\\>, dim_feedforward)\n",
    "* (training_data, \\<d_model=128\\>, dim_feedforward)\n",
    "* (training_data, \\<d_model=256\\>, dim_feedforward)\n",
    "\n",
    "So each set of plots is 13 by 4 and we can visually examine them before doing any statisical testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2016x720 with 52 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataDir = Path(\"./output\")\n",
    "count=0\n",
    "for fname in sorted(os.listdir(dataDir)):\n",
    "    if fname.endswith(\"model\"):\n",
    "        continue\n",
    "    fullPath = dataDir/Path(fname)\n",
    "    count+=1\n",
    "    #print(fullPath)\n",
    "fig, ax = plt.subplots(4, 13)\n",
    "for row in range(4):\n",
    "    for col in range(13):\n",
    "        csvName = f\"output/{int(800+col*100)}_64_{int(2**row * 256)}.csv\"\n",
    "        plt.setp(ax[row][col].get_xticklabels(), visible=False)\n",
    "        if col != 0:\n",
    "            plt.setp(ax[row][col].get_yticklabels(), visible=False)\n",
    "        df = pd.read_csv(csvName)\n",
    "fig.set_size_inches(28, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we have a base model \n",
    "* Roughly matches the paper\n",
    "* Can we do better?\n",
    "### We mentioned two methods\n",
    "* TrueGrad adam\n",
    "* Enforcing rules on the singular values\n",
    "### Lets first explore singular values\n",
    "* Relevant to the paper was that the model didn't seem to \"grok\" until the singular values became small\n",
    "* Can we enforce this somehow?\n",
    "    * Is this idea new?\n",
    "    * No idea is new...\n",
    "### Relevant lit.\n",
    "#### Paper 1\n",
    "* This [paper](https://arxiv.org/pdf/1611.06013.pdf) put a hard bound on singular values keeping them near 1\n",
    "    * This preformed reasonably well \n",
    "    * But they have a problem! They have to somewhat regularly compute an svd composition to enforce the rules on their weight matrix!\n",
    "        * This is extremely expensive!\n",
    "#### Paper 2\n",
    "* Another [paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2013/01/svd_v2.pdf) use the svd to improve sparcity for generalization\n",
    "    * This is (no pun) in some sense orthogonal to what we are doing, but an easily transferable task worth mentioning\n",
    "* There actually isn't a ton of relevant literature on enforcing singular values (although there is a lot of research on orthogonal matrices and CNN's)\n",
    "    * Why is this though?\n",
    "    * We like to believe it is because methods involved computing the svd of the weights\n",
    "    * this is prohibitively expensive\n",
    "#### Paper 3\n",
    "* But there is one very interesting paper!\n",
    "* This beautiful [paper](https://arxiv.org/pdf/2009.13977.pdf) which recieved a neurips spotlight proposed SVD neural networks\n",
    "* Essentially they represented each weight as its SVD decompositions ($U$, $S$, $V$) and trained them\n",
    "    * This would be nice... but U and V must be orthogonal for this to make any sense\n",
    "        * Regularization on gradient descent isn't enough!\n",
    "        * We need math!\n",
    "    * They represent $U$ and $V$ as a product of house holder matrices!\n",
    "        * A householder matrix is a special orthogonal matrix which *stays orthogonal on gradient descent!!!!*\n",
    "        * Limitations\n",
    "            * $U$ $S$ $V$ doesn't necessarily span the most useful space when $U$ and $V$ must be householder matrices\n",
    "        * Solution!\n",
    "            * Any orthogonal matrix $A \\in R^{(N, N)}$ can be represented by $N$ house holder matrices\n",
    "        * Limitation of solution\n",
    "            * Oh my god that turns the first layer of mnist into a minium of 784 matrix multiplications\n",
    "        * They wrote the library mentioned at the top [fasth](https://github.com/AlexanderMath/fasth) which does the math much faster\n",
    "        * Still not ideal\n",
    "### Proposed ideas\n",
    "* Okay we have some reasons to believe we might be able to accomplish the task\n",
    "    * This looks like enforcing some \"smallness\" rules on our singular values\n",
    "* Lets implement something things and test our assumptions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using householder matrices on mnist\n",
    "* Mnist is great because its very learnable\n",
    "#### Key limitation of householder\n",
    "* You can *not* use momentum optimizers\n",
    "* Householder matrices hold their properties on $SGD$ but non linear operators on the gradient break the properties\n",
    "#### Householder defintion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def householder(vector: Tensor):\n",
    "    # assert column vector, just a good sanity check\n",
    "    assert vector.shape[1] == 1\n",
    "    assert len(vector.shape) == 2\n",
    "    I = torch.eye(vector.shape[0])\n",
    "    return I - 2*vector@vector.T/(vector.T@vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of Necessary classes\n",
    "* Essentially we want a linear layer that stores the singular values\n",
    "* It's useful to abstract the house holder matrices to modules\n",
    "    * We can vary the number of house holder matrices later\n",
    "#### Glossed over but non square weight matrices are annoying\n",
    "* Defined as follows $W \\in R^{(N, M)}$\n",
    "    * $U \\in R^{(N, N)}$, $S \\in R^{(N, M)}$, $M \\in R^{(M, M)}$\n",
    "\n",
    "#### What the hell is setHouseOrthNHouseHolders!!!\n",
    "* Why I'm glad you asked\n",
    "* For reasons well beyond me I can't properly segment the parameters when using a parameter list\n",
    "* So I write a function that outputs the houseOrth as a string with the right number of parameters\n",
    "* We can then call exec on that object to set the number of house holder matrices\n",
    "* I'm as upset as you are about this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.nn import init\n",
    "#from torch import device, dtype\n",
    "# A major todo is to verify this preserves the singular values \n",
    "# It doesn't really change the \"hardness\" of the problem\n",
    "# But my math ability is questionable\n",
    "# V @ (S.T @ (U @ x.T))#\n",
    "class HouseOrth(torch.nn.Module):\n",
    "    def __init__(self, N: int, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        # random initialization... this could be improved\n",
    "        self.U = Parameter(householder(torch.empty(N,1, **factory_kwargs).uniform_(-1/math.sqrt(N), 1/math.sqrt(N))))\n",
    "        \n",
    "    def forward(self, x: Tensor):\n",
    "        \n",
    "        return self.U @ x.T\n",
    "\n",
    "def setHouseOrthNHouseHolders(n_householders):\n",
    "    \"\"\"\n",
    "    Father forgive me for I hath sinned\n",
    "    \"\"\"\n",
    "    u = [f\"\\t\\tself.U{i} = Parameter(householder(torch.empty(N,1, **factory_kwargs).uniform_(-1/math.sqrt(N), 1/math.sqrt(N))))\" for i in range(n_householders)]\n",
    "    u = '\\n'.join(u)\n",
    "    u = u.replace(\"\\t\", \" \"*4)\n",
    "    def helper(i):\n",
    "        if i == 0:\n",
    "            return \"self.U0 @ x.T\"\n",
    "        return f\"self.U{i} @ ({helper(i-1)})\"\n",
    "\n",
    "    s = f\"\"\"\n",
    "class HouseOrth(torch.nn.Module):\n",
    "    def __init__(self, N: int, device=None, dtype=None):\n",
    "        factory_kwargs = {{'device': device, 'dtype': dtype}}\n",
    "        super().__init__()\n",
    "        # random initialization... this could be improved\n",
    "{u}\n",
    "    def forward(self, x: Tensor):\n",
    "        \n",
    "        return {helper(n_householders-1)}\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    #exec(s,  )\n",
    "    return s\n",
    "\n",
    "\n",
    "class LinearSVD(torch.nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        N,M = in_features, out_features\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.U = HouseOrth(N, **factory_kwargs)\n",
    "        # we normalize the singular values in the multiplication - smart initialization probably doesn't matter too much\n",
    "        # and if it does we do not know what that would be\n",
    "        self.singulars = Parameter(torch.empty(M, **factory_kwargs).uniform_(-1/math.sqrt(out_features), 1/math.sqrt(in_features)))\n",
    "        self.register_buffer(\"S\", torch.eye(N,M, **factory_kwargs)) # singulars are trainable - the matrix isn't (only the diagonal are parameters)\n",
    "        self.V = HouseOrth(M, **factory_kwargs) # Whether or not this is transposed doesn't really matter - its valid either way just a diff matrix\n",
    "        # copy torch.nn.Linear for what its worth here\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty((out_features,), **factory_kwargs) )\n",
    "            #step = self.U()\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.V(self.U( (self.singulars*self.S).T)))\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.V( ((self.singulars * self.S).T @ self.U(x)).T ).T + self.bias\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test how well this learns on full batch mnist\n",
    "* Use simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.datasets.mnist as mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = torch.tensor(x_train).cuda().reshape(x_train.shape[0], math.prod(x_train.shape[1:])).float() #/ 255 # normalize b/c uh this is prolly smart?\n",
    "y_train = torch.tensor(y_train).cuda()\n",
    "\n",
    "x_test = torch.tensor(x_test).cuda().reshape(x_test.shape[0], math.prod(x_test.shape[1:])).float()\n",
    "y_test = torch.tensor(y_test).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, useSVD=True):\n",
    "        super().__init__()\n",
    "        if useSVD:\n",
    "            Linear = LinearSVD\n",
    "        else:\n",
    "            Linear = torch.nn.Linear\n",
    "        self.fc1 = Linear(784, 128)\n",
    "        self.fc2 = Linear(128, 128)\n",
    "        self.fc3 = Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x: Tensor):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.softmax(self.fc3(x), -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare linearSVD with Linear\n",
    "* We cannot use Adam on HouseOrth\n",
    "* We can on singulars\n",
    "* We do not care about generalization on mnist\n",
    "    * Only inspecting training accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentHouseHolderParameters(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Returns tuple[0] = HouseParameters for SGD optimizer \n",
    "    tuple[1] = All other Parameters for Adam\n",
    "    \"\"\"\n",
    "    houseParams: List[Parameter] = []\n",
    "    otherParams: List[Parameter] = []\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, HouseOrth):\n",
    "            # parameter lists... needs to be true apparently... honest to god not sure why\n",
    "            # but since HouseOrth is a base module it doesn't cause errors\n",
    "            for p in module.parameters(False): houseParams.append(p) \n",
    "        else:\n",
    "            for p in module.parameters(False): otherParams.append(p)\n",
    "    return houseParams, otherParams\n",
    "\n",
    "def getOptimizers(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Optimizers get unhappy when you give them an empty list, so we don't do that\n",
    "    Just an abstraction to avoid rewriting a lot of code\n",
    "    \"\"\"\n",
    "    hParams, nParams = segmentHouseHolderParameters(model)\n",
    "    optimizers: List[torch.optim.Optimizer] = []\n",
    "    if len(hParams) != 0:\n",
    "        optimizers.append(torch.optim.SGD(hParams, lr=3e-3))\n",
    "    if len(nParams) != 0:\n",
    "        optimizers.append(torch.optim.Adam(nParams, lr=3e-4))\n",
    "    return optimizers \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatchTrain(model: torch.nn.Module, optimizers: List[torch.optim.Optimizer], epochs: int = 100, batch_size: int = 1000) -> List[Tensor]:\n",
    "    \"\"\"\n",
    "    Train the model, return list of accuracies\n",
    "    the optimizers are a list because we want to be able to use different optimizers\n",
    "    on different parameters in the network. \n",
    "\n",
    "    Our problem doesn't do minibatching - but not minibatching makes result pretty unstable so for testing we use minibatching.\n",
    "    Note: I refuse to learn how to use pytorch's dataloader it confused me once 4 years ago and I took it personally.\n",
    "    Note: Yes the batch size is non standard but idk divisible by 60k is nice\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    samples = x_train.shape[0]\n",
    "    n_batches = samples // batch_size\n",
    "    for i in range(epochs):\n",
    "        perm = np.random.permutation(samples) # switch order every epoch generally good practice\n",
    "        for batch in range(n_batches):\n",
    "            x = x_train[perm[batch*batch_size:(batch+1)*batch_size]]\n",
    "            y = y_train[perm[batch*batch_size:(batch+1)*batch_size]]\n",
    "            for opt in optimizers: opt.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = F.cross_entropy(pred, y)\n",
    "            loss.backward()\n",
    "            for opt in optimizers: opt.step()\n",
    "        with torch.no_grad():\n",
    "            acc = (torch.argmax(model(x_train), -1) == y_train).float().mean()\n",
    "            accuracies.append(acc.clone().detach().cpu().numpy())\n",
    "    return accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal linear model\n",
    "standardModel = Model(useSVD=False).cuda()\n",
    "standardOpt = getOptimizers(standardModel)\n",
    "standardAcc = minibatchTrain(standardModel, standardOpt)\n",
    "#setHouseOrthNHouseHolders(2)\n",
    "# svd linear model\n",
    "s = setHouseOrthNHouseHolders(2)\n",
    "exec(s) # yes this is insane\n",
    "svdModel = Model(useSVD=True).cuda()\n",
    "svdOpt = getOptimizers(svdModel)\n",
    "svdAcc = minibatchTrain(svdModel, svdOpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5e6cb3d610>"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# plotting\n",
    "x = np.arange(len(standardAcc))\n",
    "ax.plot(x, standardAcc, label=\"Standard model\")\n",
    "ax.plot(x, svdAcc, label = \"SVD Model\")\n",
    "ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yea this isn't perfect!\n",
    "* But it does learn!\n",
    "* We managed to restrict the space *considerably*\n",
    "    * We can make the learnable space larger by using more house holder matrices for $U$ and $V$\n",
    "### The sane way to do this is a parameter list\n",
    "* For the love of god I don't know why this causes problems\n",
    "* But when I segment the parameters it doesn't work right...\n",
    "    * U and V loose the fact they are orthgonal\n",
    "* Solving this is insane\n",
    "    * But we perservere\n",
    "### Write a function that outputs a new HouseOrth class\n",
    "* This houseOrth class will have the correct number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0491, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(svdModel.fc1.U.U1 @ svdModel.fc1.U.U1.T - torch.eye(784).cuda()).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0805, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(svdModel.fc1.U.U @ svdModel.fc1.U.U.T - torch.eye(784).cuda()).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0675, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0713, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = svdModel.fc1.V.U1\n",
    "print((t @ t.T - torch.eye(128).cuda()).abs().max())\n",
    "t = svdModel.fc1.V.U2\n",
    "(t @ t.T - torch.eye(128).cuda()).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[     0.9970,     -0.0017,     -0.0029,  ...,      0.0021,\n",
       "              -0.0045,     -0.0004],\n",
       "         [    -0.0017,      0.9991,     -0.0016,  ...,      0.0012,\n",
       "              -0.0025,     -0.0002],\n",
       "         [    -0.0029,     -0.0016,      0.9972,  ...,      0.0021,\n",
       "              -0.0043,     -0.0004],\n",
       "         ...,\n",
       "         [     0.0021,      0.0012,      0.0021,  ...,      0.9985,\n",
       "               0.0032,      0.0003],\n",
       "         [    -0.0045,     -0.0025,     -0.0043,  ...,      0.0032,\n",
       "               0.9934,     -0.0006],\n",
       "         [    -0.0004,     -0.0002,     -0.0004,  ...,      0.0003,\n",
       "              -0.0006,      0.9999]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[     0.9865,     -0.0247,     -0.0122,  ...,      0.0009,\n",
       "               0.0089,     -0.0100],\n",
       "         [    -0.0247,      0.9551,     -0.0223,  ...,      0.0016,\n",
       "               0.0163,     -0.0183],\n",
       "         [    -0.0122,     -0.0223,      0.9890,  ...,      0.0008,\n",
       "               0.0081,     -0.0091],\n",
       "         ...,\n",
       "         [     0.0009,      0.0016,      0.0008,  ...,      0.9999,\n",
       "              -0.0006,      0.0006],\n",
       "         [     0.0089,      0.0163,      0.0081,  ...,     -0.0006,\n",
       "               0.9941,      0.0066],\n",
       "         [    -0.0100,     -0.0183,     -0.0091,  ...,      0.0006,\n",
       "               0.0066,      0.9925]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[     0.9964,     -0.0005,     -0.0111,  ...,     -0.0113,\n",
       "              -0.0062,      0.0124],\n",
       "         [    -0.0005,      0.9999,     -0.0015,  ...,     -0.0015,\n",
       "              -0.0009,      0.0017],\n",
       "         [    -0.0111,     -0.0015,      0.9660,  ...,     -0.0346,\n",
       "              -0.0190,      0.0378],\n",
       "         ...,\n",
       "         [    -0.0113,     -0.0015,     -0.0346,  ...,      0.9647,\n",
       "              -0.0194,      0.0386],\n",
       "         [    -0.0062,     -0.0009,     -0.0190,  ...,     -0.0194,\n",
       "               0.9893,      0.0212],\n",
       "         [     0.0124,      0.0017,      0.0378,  ...,      0.0386,\n",
       "               0.0212,      0.9578]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.9606,  0.0219, -0.0083,  ...,  0.0231,  0.0328, -0.0232],\n",
       "         [ 0.0219,  0.9878,  0.0046,  ..., -0.0129, -0.0182,  0.0129],\n",
       "         [-0.0083,  0.0046,  0.9983,  ...,  0.0048,  0.0069, -0.0049],\n",
       "         ...,\n",
       "         [ 0.0231, -0.0129,  0.0048,  ...,  0.9864, -0.0192,  0.0136],\n",
       "         [ 0.0328, -0.0182,  0.0069,  ..., -0.0192,  0.9727,  0.0193],\n",
       "         [-0.0232,  0.0129, -0.0049,  ...,  0.0136,  0.0193,  0.9863]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[     0.9669,      0.0240,      0.0074,  ...,     -0.0348,\n",
       "               0.0076,     -0.0018],\n",
       "         [     0.0240,      0.9827,     -0.0054,  ...,      0.0251,\n",
       "              -0.0055,      0.0013],\n",
       "         [     0.0074,     -0.0054,      0.9983,  ...,      0.0078,\n",
       "              -0.0017,      0.0004],\n",
       "         ...,\n",
       "         [    -0.0348,      0.0251,      0.0078,  ...,      0.9635,\n",
       "               0.0080,     -0.0019],\n",
       "         [     0.0076,     -0.0055,     -0.0017,  ...,      0.0080,\n",
       "               0.9982,      0.0004],\n",
       "         [    -0.0018,      0.0013,      0.0004,  ...,     -0.0019,\n",
       "               0.0004,      0.9999]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.7229, -0.1477, -0.3198, -0.2354,  0.2744, -0.2801, -0.0837, -0.1967,\n",
       "           0.3137,  0.0069],\n",
       "         [-0.1477,  0.9212, -0.1705, -0.1255,  0.1463, -0.1493, -0.0446, -0.1049,\n",
       "           0.1673,  0.0037],\n",
       "         [-0.3198, -0.1705,  0.6309, -0.2717,  0.3167, -0.3232, -0.0966, -0.2270,\n",
       "           0.3621,  0.0079],\n",
       "         [-0.2354, -0.1255, -0.2717,  0.8000,  0.2331, -0.2379, -0.0711, -0.1671,\n",
       "           0.2665,  0.0058],\n",
       "         [ 0.2744,  0.1463,  0.3167,  0.2331,  0.7282,  0.2773,  0.0829,  0.1948,\n",
       "          -0.3107, -0.0068],\n",
       "         [-0.2801, -0.1493, -0.3232, -0.2379,  0.2773,  0.7170, -0.0846, -0.1987,\n",
       "           0.3171,  0.0070],\n",
       "         [-0.0837, -0.0446, -0.0966, -0.0711,  0.0829, -0.0846,  0.9747, -0.0594,\n",
       "           0.0948,  0.0021],\n",
       "         [-0.1967, -0.1049, -0.2270, -0.1671,  0.1948, -0.1987, -0.0594,  0.8604,\n",
       "           0.2227,  0.0049],\n",
       "         [ 0.3137,  0.1673,  0.3621,  0.2665, -0.3107,  0.3171,  0.0948,  0.2227,\n",
       "           0.6448, -0.0078],\n",
       "         [ 0.0069,  0.0037,  0.0079,  0.0058, -0.0068,  0.0070,  0.0021,  0.0049,\n",
       "          -0.0078,  0.9998]], requires_grad=True)]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentHouseHolderParameters(Model(useSVD=True))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "s =\"\"\"\n",
    "class Dummy:\n",
    "    pass\n",
    "\"\"\"\n",
    "print(exec(s.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Dummy at 0x7f5e6cde2dc0>"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('cmsc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84632eeb22439882512e471b7249f8dc904dd5464b6101158ce571edabf38baa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
