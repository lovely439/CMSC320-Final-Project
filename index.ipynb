{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Max said he would grade this\n",
    "# Optimal grokking: Exploring TrueGrad Adam implementations and normalized singular values\n",
    "\n",
    "## Introduction\n",
    "What is grokking? [Grokking](https://arxiv.org/pdf/2201.02177.pdf) is a curious phenomenon in neural networks. It in some sense spits in the face of classic understanding of training models. Rather than describe it, as they say, a picture is worth 1,000 words.\n",
    "\n",
    "</br>\n",
    "\n",
    "![](./grokking.png)\n",
    "\n",
    "Taken from the research paper linked earlier, grokking is the behavior observed on some simple problems that long past overfitting, models almost suddenly generalize. And by long past overfitting, in this example we mean *long* past, nearly 4 orders of magnitude!\n",
    "\n",
    "## Its not actually that bad!\n",
    "Okay, so 4 orders of magnitude seem completely untenable, but this was the worst case described in the paper. Lets talk about the problem discussed in the paper a bit.\n",
    "</br>\n",
    "\n",
    "### So what is the problem described in the paper? What does the paper do?\n",
    "### The problem (simplified)\n",
    "Modular division </br>\n",
    "* Choose some prime $P$, the paper chose $P=97$\n",
    "* Generate all equations of the form $a+b \\equiv c$ (mod $P)$\n",
    "    * $a,b,c \\in \\mathbb{Z}^{0\\leq P}$\n",
    "* This will generate $N_{data} = P*P$ equations\n",
    "    * Split the data into training and validating\n",
    "* Train a standard TransformerDecoder of the following structure\n",
    "    * Embedding layer with positional encoding (functionally an encoder)\n",
    "    * 2 Layers\n",
    "        * width 128\n",
    "        * 4 attention heads\n",
    "* With an Adam optimizer having parameters\n",
    "    * learning rate $10^{-3}$\n",
    "    * weight decay $10^{-2}$ - they said 1, but pretty sure they meant $1e-2$... 1 is insane\n",
    "    * $B_1 = 0.9$, $B_2=0.98$\n",
    "\n",
    "## The problem actually described in the paper\n",
    "The problem described above is a lemma of what the paper actually does. This section is not required to understand anything we did, but we would be remiss if we did not talk about the finer details of the paper. </br>\n",
    "* Not just modular division\n",
    "    * $a \\circ b \\equiv c$(mod $P$) \n",
    "    * For the following ops\n",
    "        *  $a \\circ b$ $ = a + b$ (mod $P$) for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = a- b$ (mod $P$) for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = [a/b$ (mod $P$) for $0 \\leq a \\le P$, $0 \\le b \\le P$\n",
    "        *  $a \\circ b$ $ = [a/b$ (mod $P$) if $b$ is odd, otherwise $a-b$ (mod $P$)] for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = a^2 + b^2$ (mod $P$) for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = a^2 + ab + b^2$ (mod $P$) for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = a^2 + ab + b^2 + a$ (mod $P$) for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = a^3 + ab$ (mod $P$) for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = a^3 + ab^2 + b$ (mod $P$) for $0 \\leq a,b \\le P$\n",
    "        *  $a \\circ b$ $ = a * b$ for $a,b \\in \\mathbb{S}^5$\n",
    "        *  $a \\circ b$ $ = a * b * a^{-1}$ for $a,b \\in \\mathbb{S}^5$\n",
    "        *  $a \\circ b$ $ = a * b * a$ for $a,b \\in \\mathbb{S}^5$\n",
    "* This means every input was 5 tokens\n",
    "    * \"a\", \"op\", \"b\", \"=\", \"c\"\n",
    "* Generate all equations of this form for some specific op\n",
    "* Convert the 5 char arrays to 5 int arrays\n",
    "* Split the data as before\n",
    "* The transformer structure is unchanged\n",
    "* They used a variety of optimization techniques and hyperparameter tuning\n",
    "    * Minibatching and full batching\n",
    "    * 10 warm up updates of mini batch size $[512, \\frac{N_{trainingData}}{2}]$\n",
    "    * optimization budget of $10^5$ gradient updates - I read this as steps.\n",
    "    * learning rate $3e-4$, $3e-3$\n",
    "    * weight decay same as before\n",
    "    * Gaussian noise on weights\n",
    "    * residual dropout 0.1\n",
    "* They also put outliers in the dataset to see how this effected grokking\n",
    "\n",
    "Importantly this reperesents a more full scope of what the study did. </br>\n",
    "For our purposes we will just take some of their better presets as a base model.\n",
    "\n",
    "## Results figure\n",
    "![](./grokkingResults.png)\n",
    "* Yes this is the worst graph in terms of labeling ever\n",
    "* No the figure explanation does not help\n",
    "* Below we talk about the key findings of the paper mostly ignoring this graph\n",
    "\n",
    "## Key Results\n",
    "* Adam is seemingly very important to grokking, at least momentum optimizers\n",
    "* Grokking didn't happen until the singular values of weights became small\n",
    "* Minibatching is superior\n",
    "* Weight decay is *extremely* *extremely* important\n",
    "* They didn't give precise results as far as We could tell\n",
    "    * No statement that is this problem, this training data split, N optimization steps to train, M optimizations steps to grok\n",
    "    * There exists some parameters such that grokking happens within an order of magnitude\n",
    "\n",
    "\n",
    "\n",
    "## What we took from this as our base model\n",
    "* The transformer model with dropout\n",
    "* Adam optimizer \n",
    "    * $lr = 3e-4$\n",
    "    * $weightDecay = 1e-2$\n",
    "    * $B_1 = 0.9$, $B_2=0.98$\n",
    "* Full batch training\n",
    "\n",
    "Essentially this represented their best presets minus minibatching which preformed significantly better in their training. \n",
    "\n",
    "\n",
    "\n",
    "### Why didn't we do minibatching?\n",
    "* We believe on intuition that TrueGrad Adam should be much more stable\n",
    "* The best way to show this is to use the \"worst\" case for the optimizer\n",
    "* Still good presets for Adam, but a hard situation\n",
    "\n",
    "## Paper Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used libraries for complete implementation\n",
    "* [pytorch](https://github.com/pytorch/pytorch)\n",
    "* [Tensorflow](https://github.com/tensorflow/tensorflow) (Just for mnist dataset)\n",
    "* [numpy](https://github.com/numpy/numpy)\n",
    "* [TrueGrad](https://github.com/ClashLuke/TrueGrad)\n",
    "* [fasth](https://github.com/AlexanderMath/fasth)\n",
    "* [pandas](https://github.com/pandas-dev/pandas)\n",
    "* [matplotlib](https://matplotlib.org/)\n",
    "#### All neccessary imports for paper re-implementation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Optional, List, Tuple\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preset parameters from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 97\n",
    "DEVICE = \"cuda\"\n",
    "D_MODEL = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "\n",
    "Unlike the paper we generate the smaller verison so. So our data looks as follows\n",
    "* $x \\in x_{train}$ x = \\[10, 17, 0\\]\n",
    "* corresponding y = \\[10, 17, (10 + 17) % p \\]\n",
    "* Essentially x is just y without the answer\n",
    "We split the data into training and validation to the specified amount of training data.\n",
    "* $x_{train}, y_{train}$ are shape (n_training_data, 3)\n",
    "* $x_{test}, y_{test}$ are shape ($P*P$ - n_training_data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(n_training_data: int, p: int, device: str = \"cuda\"):\n",
    "    # generate all possible equations for mod p\n",
    "    all_data = []\n",
    "    for i in range(p):\n",
    "        for j in range(p):\n",
    "           all_data.append([i,j,(i+j)%p])\n",
    "    \n",
    "    all_data = np.array(all_data)\n",
    "    indices = np.random.permutation(all_data.shape[0])\n",
    "    train_indices = indices[:n_training_data]\n",
    "    valid_indices = indices[n_training_data:]\n",
    "    \n",
    "    input_seq: np.ndarray = all_data.copy()\n",
    "    output_seq = input_seq.copy()\n",
    "    input_seq[:, -1] = 0 # don't include answers\n",
    "\n",
    "    train_x = Tensor(input_seq[train_indices]).long().to(device)\n",
    "    train_y = Tensor(output_seq[train_indices]).long().to(device)\n",
    "    valid_x = Tensor(input_seq[valid_indices]).long().to(device)\n",
    "    valid_y = Tensor(output_seq[valid_indices]).long().to(device)\n",
    "\n",
    "    return (train_x, train_y), (valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The embedding layer\n",
    "Pytorch doesn't seem to have an embedding with position encoding built in yet. It might seem overkill to implement but transformers are permutation invariant which on a symmetric problem might give us an unfair advantage.\n",
    "\n",
    "Our implementation is standard or positional encoding - we used the [following](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/) as reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithPE(torch.nn.Embedding):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, seq_len: int, N: int= 10_000,\n",
    "            padding_idx: Optional[int] = None, max_norm: Optional[float] = None,\n",
    "            norm_type: float = 2.0, scale_grad_by_freq: bool = False, sparse: bool=False,\n",
    "            device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None\n",
    "        ):\n",
    "        super().__init__(\n",
    "            num_embeddings, embedding_dim, padding_idx, max_norm, norm_type,\n",
    "            scale_grad_by_freq, sparse, device, dtype\n",
    "        )\n",
    "       \n",
    "        pe = torch.zeros((seq_len, embedding_dim))\n",
    "        buf = torch.arange(embedding_dim//2)\n",
    "        denom = torch.pow(N, (2*buf)/embedding_dim)\n",
    "        k = torch.arange(seq_len)\n",
    "        pe[k, ::2] = torch.sin(k.unsqueeze(1)/denom)\n",
    "        pe[k, 1::2] = torch.cos(k.unsqueeze(1)/denom)\n",
    "        # pe should not be a learnable parameter, a buffer not a parameter\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "             \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return F.embedding(\n",
    "            x, self.weight, self.padding_idx, self.max_norm,\n",
    "            self.norm_type, self.scale_grad_by_freq, self.sparse) + self.pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer Model\n",
    "Won't rehash the copying of the paper but will mention a few important things.\n",
    "The paper stipulated a standard decoder so this is what we built, this includes masking and layernorms.\n",
    "\n",
    "Also important is a decoder of width N will have an output layer with N neurons. We are classifying a number that is at most P which is num_embeddings. So we add a layer such that the final layer has num_embedding (P) outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, seq_length: int, num_embeddings: int, d_model: int, dim_feedforward: int = 2048):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embed = EmbeddingWithPE(\n",
    "            self.num_embeddings, self.d_model, seq_length)\n",
    "\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(self.d_model, nhead=4, dim_feedforward=dim_feedforward,\n",
    "                                        dropout=0.1, batch_first=True, norm_first=True)\n",
    "        decoder_norm = torch.nn.LayerNorm(self.d_model)\n",
    "\n",
    "        self.decoder = torch.nn.TransformerDecoder(\n",
    "            decoder_layer, num_layers=2, norm=decoder_norm)\n",
    "        self.linear = torch.nn.Linear(\n",
    "            self.d_model, self.num_embeddings, bias=False)\n",
    "        self.mask = torch.nn.Parameter(\n",
    "            torch.ones([seq_length, seq_length]).tril())\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.embed(x)\n",
    "        x = self.decoder.forward(x, torch.zeros_like(x), self.mask)\n",
    "        return self.linear(x)\n",
    "\n",
    "    def acc(self, prediction: Tensor, labels: Tensor):\n",
    "        return (torch.argmax(prediction, dim=-2) == labels).float().mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "Here we write a simple training loop to do full batch training of the model. We record the training losses and accuracie as well as the validation losses and accuracies. \n",
    "\n",
    "We use cross entropy because always use cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_train(\n",
    "    model: Transformer,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_x: Tensor,\n",
    "    train_y: Tensor, \n",
    "    valid_x: Tensor, \n",
    "    valid_y: Tensor,\n",
    "    epochs: int = 10_000,\n",
    "    quiet: bool = False\n",
    ") -> Tuple[ Tuple[List[Tensor], List[Tensor]],  Tuple[List[Tensor], List[Tensor]]  ]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Simple training method, does full batch training on transformer model\n",
    "    returns (train_accs, train_losses), (valid_accs, valid_losses) \n",
    "    where each list is length epochs\n",
    "    \"\"\"\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_losses: List[Tensor] = []\n",
    "    train_accs: List[Tensor] = []\n",
    "    validation_losses: List[Tensor] = []\n",
    "    validation_accs: List[Tensor] = []\n",
    "    for i in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(train_x)\n",
    "        print(output.shape)\n",
    "        # model.forward shape is (n_inputs, 3, 97)\n",
    "        # transpose to (n_inputs, 97, 3)\n",
    "        pred = output.transpose(-2, -1)\n",
    "        # -1: to not drop the dim\n",
    "        pred = pred[..., -1:] \n",
    "        label = train_y[:, -1:]\n",
    "\n",
    "        loss = F.cross_entropy(pred, label)\n",
    "        acc = model.acc(pred, label)\n",
    "        \n",
    "        # keep track of losses\n",
    "        train_accs.append(acc.item())\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_valid = model.forward(valid_x).transpose(-2, -1)[..., -1:]\n",
    "            label_valid = valid_y[:, -1:]\n",
    "            valid_loss = F.cross_entropy(pred_valid, label_valid)\n",
    "            valid_acc = model.acc(pred_valid, label_valid)\n",
    "            validation_accs.append(valid_acc.item())\n",
    "            validation_losses.append(valid_loss.item())\n",
    "        if i % 100 == 0 and not quiet:\n",
    "            print(f\"Epoch {i}: loss {loss.item():e}, training_accuracy {acc}, valid_acc {valid_acc:4f}, valid_loss: {valid_loss:4f}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return (train_accs, train_losses), (validation_accs, validation_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on a variety of parameters\n",
    "\n",
    "We want to generate an amount of results to compare so we do grid search on the following parameters.\n",
    "* N_epochs = 15,000\n",
    "* n_training_data = 800 + 100k $k \\in \\mathbb{Z}^{0 \\le 12}$\n",
    "* d_model = \\[64, 128, 256\\]\n",
    "* dim_feedforward = \\[256, 512, 1024, 2048\\]\n",
    "</br>\n",
    "\n",
    "This took 1.5 *days* of compute on an rtx 2060 12GB.\n",
    "We save all fully trained models to do later analysis on their singular values as well the train and validation losses and accuracies.\n",
    "\n",
    "The naming scheme for the saved files is\n",
    "* .csv or .model depending on if its the model state dict or the training accuracies losses etcetera\n",
    "* \\<n_training_data\\>_\\<d_model\\>_\\<dim_feedforward\\>\n",
    "* To not confuse anyone from earlier d_model is the same as width from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of not running this biaccident we put this behind a function\n",
    "def DO_NOT_RUN():\n",
    "    for n_training_data in range(800, 2001, 100):\n",
    "        # For consistency training data must be the same for the models of different params\n",
    "        (train_x, train_y), (valid_x, valid_y) = gen_data(n_training_data, P, device=DEVICE)\n",
    "\n",
    "        for d_model in [64, 128, 256]:\n",
    "            for dim_feedforward in [256, 512, 1024, 2048]:\n",
    "                model = Transformer(train_x.size(-1), P, d_model, dim_feedforward=dim_feedforward).to(DEVICE)\n",
    "\n",
    "                optimizer = torch.optim.Adam(\n",
    "                    model.parameters(),\n",
    "                    lr=3e-4,\n",
    "                    betas=[.9, .98],\n",
    "                    weight_decay=1e-2,\n",
    "                    eps=1e-8,\n",
    "                    amsgrad=False,\n",
    "                )\n",
    "                print(f\"Training model: n_train_data: {n_training_data}, d_model: {d_model} dim_feedforward: {dim_feedforward} \")\n",
    "                (acc, loss), (valid_acc, valid_loss) = simple_train(model, optimizer, train_x, train_y, valid_x, valid_y, quiet=False,epochs=15_000)\n",
    "                df_cols = [\"Training Accuracy\", \"Training Loss\", \"Validation Accuracy\", \"Validation Loss\"]\n",
    "                df = pd.DataFrame(list(zip(acc, loss, valid_acc, valid_loss)), columns=df_cols)\n",
    "                df.to_csv(f\"output/{n_training_data}_{d_model}_{dim_feedforward}.csv\")\n",
    "                torch.save(model.state_dict(), f\"output/{n_training_data}_{d_model}_{dim_feedforward}.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of accuracies\n",
    "\n",
    "First lets make a series plots as follows where <> denotes an immutable parameter\n",
    "* (training_data, \\<d_model=64\\>, dim_feedforward)\n",
    "* (training_data, \\<d_model=128\\>, dim_feedforward)\n",
    "* (training_data, \\<d_model=256\\>, dim_feedforward)\n",
    "\n",
    "So each set of plots is 13 by 4 and we can visually examine them before doing any statisical testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2016x720 with 52 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataDir = Path(\"./output\")\n",
    "count=0\n",
    "for fname in sorted(os.listdir(dataDir)):\n",
    "    if fname.endswith(\"model\"):\n",
    "        continue\n",
    "    fullPath = dataDir/Path(fname)\n",
    "    count+=1\n",
    "    #print(fullPath)\n",
    "fig, ax = plt.subplots(4, 13)\n",
    "for row in range(4):\n",
    "    for col in range(13):\n",
    "        csvName = f\"output/{int(800+col*100)}_64_{int(2**row * 256)}.csv\"\n",
    "        plt.setp(ax[row][col].get_xticklabels(), visible=False)\n",
    "        if col != 0:\n",
    "            plt.setp(ax[row][col].get_yticklabels(), visible=False)\n",
    "        df = pd.read_csv(csvName)\n",
    "fig.set_size_inches(28, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('cmsc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84632eeb22439882512e471b7249f8dc904dd5464b6101158ce571edabf38baa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
